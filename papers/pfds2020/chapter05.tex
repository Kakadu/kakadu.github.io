\begin{frame}{Самая простая структура данных: связный список}
\begin{definition}[Связный список]
... вы же знаете, да?

\end{definition}

\begin{definition}[Список]
\uncover<2->{
Структура данных, у которой для некоторой выбранной стороны (например, голова списка) добавление и удаление элементов работает из O(1)
}
\end{definition}
\end{frame}

\begin{frame}{Конкатенация связных списков в императивной среде}
\begin{figure}[h]
  \includegraphics{tikzpics/fig.2.4.before.pdf}\par
	(до)\par\vspace{1em}
  \includegraphics{tikzpics/fig.2.4.after.pdf}\par
	(после)\par
	\caption{Выполнение конкатенации списков \mlinline{xs} и \mlinline{ys}  в императивной среде. Эта операция уничтожает списки-аргументы \texttt{xs} и \texttt{ys} (их использовать больше нельзя)}
	\label{fig:2.4}
\end{figure}
\end{frame}


%\begin{frame}[fragile]{Конкатенация в функциональной среде}
%В функциональной среде мы не можем деструктивно модифицировать. Поэтому
%\begin{itemize}
%\item добавляем последний элемент первого списка ко второму списку
%\item добавляем \emph{пред}последний элемент первого списка к результату
%\item и т.д.
%\end{itemize}
%\vspace{2em}
%
%\begin{minted}{ocaml}
%let rec append xs ys = 
%  match xs with 
%  | [] -> ys 
%  | x::xs -> x::(append xs ys)
%\end{minted}
%\end{frame}


\begin{frame}{Конкатенация чисто функциональных списков}
\begin{figure}[h]
	\centering
  \includegraphics{tikzpics/fig.2.5.before.pdf}\par
	(до)\par
	\vspace{0.5cm}
  \includegraphics{tikzpics/fig.2.5.after.pdf}\par
	(после)\par
	\vspace{0.5cm}
	\caption{Выполнение \texttt{zs = xs ++ ys} в функциональной среде. Заметим, что списки-аргументы \texttt{xs} и \texttt{ys} не затронуты операцией.
	}
	\label{fig:2.5}
\end{figure}
Несмотря на большой объем копирования, заметим, что второй список копировать не пришлось
\end{frame}


\begin{frame}{}
\textbf{Как реализовать конкатенацию  \mlinline{++} списков \mlinline{xs} и \mlinline{ys}?}
\begin{itemize}
\item Если \mlinline{xs} пустой, то \mlinline{ys} -- ответ
\item Иначе \mlinline{xs} состоит из головы \mlinline{h} и хвоста \mlinline{tl}, а ответ -- это прицепление головы \mlinline{h} к хвосту  \mlinline{tl++ys}
\end{itemize}
Сложность: O(length(xs))
\vspace{2em}

\textbf{Как сложно обращаться к n-му элементу?}

Ответ: O(n), что несколько печалит
%\vspace{2em}

%Потом улучшим
\end{frame}

\begin{frame}{Ассоциативность конкатенации}
В теории конкатенация ассоциативна
\[
  (((a_1 \mdoubleplus a_2) \mdoubleplus a_3) \mdoubleplus\dots \mdoubleplus a_n) \equiv 
  (a_1 \mdoubleplus (a_2 \mdoubleplus (a_3 \mdoubleplus (\dots \mdoubleplus a_n))))
\]
На практике то, что слева будет работать сильно медленнее того, что справа.

\begin{hint}
Иногда, для эффективной реализации надо переписывать алгоритмы, чтобы короткие списки конкатенировались с длинными. В идеале: всегда конкатенировать один элемент с длинным списком.
\end{hint}
\end{frame}



\subsection{Методы амортизированного анализа}
\label{sc:5.1}


\begin{frame}{Методы амортизированного анализа}
Стандартная нотация для сложности $O(\cdot)$ -- оценка в худшем случае
\vspace{2em}

Но мы можем себе позволить большую свободу:
\begin{itemize}
\item Будем делать $n+1$ действий
\item Большинство действий будет дешёвыми: $O(1)$
\item Одно будет дорогим: например, $(O(n))$
\item Стандартная асимптотическая сложность будет $(O(n))$
\item Сложность \textit{в среднем} при исполнении $n$ операций (\textit{амортизированная сложность}) вполне может быть $O(1)$ за одну операцию
\end{itemize}
\vspace{2em}

%Чтобы совместить амортизацию и устойчивость стоит применить 
%\emph{ленивые вычисления}.

% Например, имея $n$
%операций, мы можем желать, чтобы время всей последовательности было
%ограничено показателем $O(n)$, не настаивая, чтобы каждая из этих
%операций происходила за время $O(1)$. Нас может устраивать, чтобы
%некоторые из операций занимали $O(\log n)$ или даже $O(n)$, при
%условии, что общая стоимость всей последовательности будет
%$O(n)$. \\

Такая дополнительная степень свободы иногда позволяет спроектировать более простую и эффективную реализацию

\end{frame}


%\begin{frame}[fragile]{}
%Амортизированная стоимость -- верхняя граница реальной
%\[
%\sum_{i=1}^m a_i \ge \sum_{i=1}^m t_i
%\]
%где $a_i$~--- амортизированная стоимость $i$-й операции, $t_i$~--- ее
%реальная стоимость, а $m$~--- общее число операций.\\
%
%Обычно
%доказывается несколько более сильный результат: что на любой
%промежуточной стадии в последовательности операций общая текущая
%амортизированная стоимость является верхней границей для общей текущей
%реальной стоимости
%\[
%\forall j: \quad \sum_{i=1}^j a_i \ge \sum_{i=1}^j t_i
%\]
%\end{frame}


\begin{frame}{Как оценивать амортизированную сложность? Метод банкира}
\begin{definition}[\term{Текущие накопления}{accumulated savings}]
Разница между общей текущей амортизированной стоимостью
и общей текущей реальной стоимостью. 
\end{definition}
\vspace{2em}
Таким образом, общая
текущая амортизированная стоимость является верхней границей для
общей текущей реальной стоимости тогда и только тогда, когда текущие
накопления неотрицательны.
\vspace{2em}

 Главное при доказательстве
амортизированных характеристик стоимости~--- показать, что дорогие
операции случаются только тогда, когда текущих накоплений хватает,
чтобы покрыть их дополнительную стоимость.

\end{frame}


%\begin{frame}[fragile]{}
%Амортизация позволяет некоторым операциям быть дороже, чем их
%амортизированная стоимость. Такие операции называются
%\term{дорогими}{expensive}. Операции, для которых амортизированная
%стоимость превышает реальную, называются
%\term{дешевыми}{cheap}. Дорогие операции уменьшают текущие накопления,
%а дешевые их увеличивают.\\
%
% Главное при доказательстве
%амортизированных характеристик стоимости~--- показать, что дорогие
%операции случаются только тогда, когда текущих накоплений хватает,
%чтобы покрыть их дополнительную стоимость.
%\end{frame}



%\begin{frame}{}
%\begin{itemize}
%  \item \term{Метод банкира}{banker's method} 
%      \begin{itemize}
%        \item \term{кредит}{credits}
%      \end{itemize}
%  \item \term{Метод физика}{physicist's method}
%      \begin{itemize}
%        \item \term{потенциал}{potential}
%      \end{itemize}
%\end{itemize}
%
%Кредит и потенциал являются лишь средствами анализа; ни
%то, ни другое не присутствует в тексте программы (разве что, возможно,
%в комментариях).
%\end{frame}

\begin{frame}{}
 В методе банкира
текущие накопления представляются как \term{кредит}{credits},
привязанный к определенным ячейкам структуры данных. Этот кредит
используется, чтобы расплатиться за будущие операции доступа к этим
ячейкам.  Амортизированная стоимость операции определяется как ее
реальная стоимость плюс размер кредита, выделяемого этой операцией,
минус размер кредита, который она расходует, т.~е.,
$$
a_i = t_i + c_i - \bar{c}_i
$$
где $t_i$~--- реальная стоимость, $c_i$~--- размер кредита, выделяемого операцией $i$, а $\bar{c}_i$~---
размер кредита, расходуемого операцией $i$.
%
%
%\end{frame}
%
%\begin{frame}[fragile]{}
%$$
%a_i = t_i + c_i - \bar{c}_i
%$$
%где $c_i$~--- размер кредита, выделяемого операцией $i$, а $\bar{c}_i$~---
%размер кредита, расходуемого операцией $i$.\\
\vspace{2em}

\begin{itemize}
\item Каждая единица кредита должна быть выделена, прежде чем  израсходована
\item Нельзя расходовать кредит дважды
\end{itemize}
\vspace{1em}

Таким образом, $\sum c_i \ge \sum \bar{c}_i$, а
следовательно, как и требуется, $\sum a_i \ge \sum t_i$.


%Как правило,
%доказательства с использованием метода банкира определяют
%\term{инвариант кредита}{credit invariant}, регулирующий распределение
%кредита так, чтобы при всякой дорогой операции достаточное его
%количество было выделено в нужных ячейках структуры для покрытия
%стоимости операции.
\end{frame}

%\begin{frame}[fragile]{Метод физика}
%Определяется функция $\Phi$, отображающая всякий
%объект $d$ на действительное число, называемое его
%\term{потенциалом}{potential}.  Потенциал обычно выбирается так, чтобы
%изначально равняться нулю и оставаться неотрицательным. В таком случае
%потенциал представляет нижнюю границу  текущих накоплений.\\
%
%Пусть объект $d_i$ будет результатом операции $i$ и аргументом
%операции $i+1$. Тогда амортизированная стоимость операции $i$
%определяется как сумма реальной стоимости и изменения потенциалов между
%$d_{i-1}$ и $d_i$, т.~е.,
%$$
%a_i = t_i + \Phi(d_i) - \Phi(d_{i-1})
%$$
%текущих накоплений.
%\end{frame}


%\begin{frame}[fragile]{}
%$$
%a_i = t_i + \Phi(d_i) - \Phi(d_{i-1})
%$$
%Текущая реальная стоимость последовательности операций равна
%$$
%\begin{array}{rcl}
%\sum_{i=1}^j t_i & = & \sum_{i=0}^j (a_i + \Phi(d_{i-1}) - \Phi(d_i))\\
%\\
%& = & \sum_{i=1}^j a_i + \sum_{i=1}^j (\Phi(d_{i-1}) - \Phi(d_i)) \\
%\\
%& = & \sum_{i=1}^j a_i + \Phi(d_0) - \Phi(d_j)
%\end{array}
%$$
%
%Если $\Phi$ выбран таким образом, что
%$\Phi(d_0)$ равен нулю, а $\Phi(d_j)$ неотрицателен, мы имеем
%$\Phi(d_j) \ge \Phi(d_0)$, так что, как и требуется, текущая общая
%амортизированная стоимость является верхней границей для текущей общей
%реальной стоимости.
%\end{frame}

\begin{comment}
\begin{remark}
Такое описание метода физика несколько упрощает
картину. Часто при анализе оказывается трудно втиснуть реальное
положение дел в указанные рамки. Например, что делать с функциями,
которые порождают или возвращают более одного объекта? Однако даже
упрощенное описание достаточно для демонстрации основных идей.
\end{remark}

\end{comment}

\subsection{Чисто функциональные очереди и их амортизация}


\begin{frame}[fragile]{Чисто функциональные очереди}
\begin{minipage}{.49\textwidth}
Интерфейс:
\begin{itemize}
\item \mlinline{empty: queue -> bool}
\item \mlinline{enqueue: queue * int -> queue}\\
добавление в очередь
\item \mlinline{head: queue -> int}\\
посмотреть на головной элемент
\item \mlinline{tail: queue -> queue}\\
изъять головной элемент
\end{itemize}
\end{minipage}
\begin{minipage}{.5\textwidth}
  Самая распространенная чисто функциональная реализация очередей
  представляет собой пару списков, \mlinline{f} и \mlinline{r}, 
  \begin{itemize}
  \item \mlinline{f} (front) содержит головные элементы очереди в правильном порядке,
  \item \mlinline{r} (reversed) состоит из хвостовых элементов в обратном порядке
  \end{itemize}
  \vspace{1em}
  
  Например, очередь, содержащая целые числа \mlinline{f=[1;2;3;4;5;6]}, может быть
  представлена списками \mlinline{f=[1;2;3]} и
  \mlinline{r=[6;5;4]}.
\end{minipage}
\end{frame}


%\begin{frame}[fragile]{}
%В этом представлении голова очереди~--- первый элемент \hsinline{f},
%так что функции \hsinline{head} и \hsinline{tail}
%возвращают и отбрасывают этот элемент, соответственно.
%\begin{minted}{haskell}
%head (x : f, r) = x
%tail (x : f, r) = f
%\end{minted}
%Подобным образом, хвостом очереди является первый элемент
%\hsinline{r}, так что \hsinline{snoc} добавляет к \hsinline{r}
%новый.
%\begin{minted}{haskell}
%snoc (f,r) x = (f, x : r)
%\end{minted}
%
%\end{frame}


\begin{frame}{Инвариант очереди}
Элементы добавляются к \mlinline{r} и убираются из \mlinline{f}, так
что они должны как-то переезжать из одного списка в другой. Этот
переезд осуществляется путем обращения \mlinline{r} и установки его
на место \mlinline{f} всякий раз, когда в противном случае
\mlinline{f} оказался бы пустым.\\

\begin{definition}[Инвариант очереди]
Список \mlinline{f} может быть пустым только в том
случае, когда список \mlinline{r} также пуст (т.~е., пуста вся
очередь).
\end{definition}
\vspace{1em}

Заметим, что если бы \mlinline{f} был пустым при непустом
\mlinline{r}, то первый элемент очереди находился бы в конце
\mlinline{r}, и доступ к нему занимал бы $O(n)$ времени. Поддерживая
инвариант, мы гарантируем, что функция \mlinline{head} всегда может
найти голову очереди за $O(1)$ времени.

\end{frame}


\begin{frame}[fragile]{Добавление и удаление из очереди}
Функция удаления из очереди \mlinline{tail: queue -> queue}
принимает очередь как пару  списков \mlinline{f} и \mlinline{r}
\begin{itemize}
\item Если  \mlinline{f} пуст -- ошибка
\item Если \mlinline{f} состоит из одного элемента \mlinline{x}, то возвращаем пару из  \mlinline{reverse(r)} и пустого списка
\item Если \mlinline{f} состоит из головного элемента \mlinline{x} и хвоста \mlinline{tl}, то возвращаем пару \mlinline{tl} и списка \mlinline{r}
\end{itemize}
\vspace{1em}

Функции
\mlinline{enqueue} и \mlinline{head} всегда завершаются за время
$O(1)$, но \mlinline{tail} в худшем случае отнимает $O(n)$
времени. \\

Однако, используя либо метод банкира, мы
можем показать, что как \mlinline{enqueue}, так и \mlinline{tail}
занимают амортизированное время $O(1)$.
\end{frame}


\begin{frame}{Чисто функциональная очередь и метод банкира}
\begin{definition}[Инвариант]
Каждый элемент в хвостовом списке связан с одной единицей кредита.
\end{definition}

Каждый вызов
\mlinline{enqueue} для непустой очереди занимает один реальный шаг и
выделяет одну единицу кредита для элемента хвостового списка; таким
образом, общая амортизированная стоимость равна двум. \\

Вызов
\mlinline{tail}, не обращающий хвостовой список, занимает один шаг,
не выделяет и не тратит никакого кредита, и, таким образом, имеет
амортизированную стоимость 1. \\

Наконец, вызов \mlinline{tail},
обращающий хвостовой список, занимает $(m+1)$ реальных шагов, где $m$~---
длина хвостового списка, и тратит $m$ единиц кредита, содержащиеся в
этом списке, так что амортизированная стоимость получается $m + 1 - m
= 1$.
\end{frame}


%\begin{frame}{Чисто функциональная очередь и метод физика}
%В методе физика мы определяем функцию потенциала $\Phi$ как длину
%хвостового списка. \\
%
%Тогда всякий \hsinline{snoc} к непустой очереди
%занимает один реальный шаг и увеличивает потенциал на единицу, так что
%амортизированная стоимость равна двум. \\
%
%Вызов \hsinline{tail} без
%обращения хвостовой очереди занимает один реальный шаг и не изменяет
%потенциал, так что амортизированная стоимость равна одному.\\
%
% Наконец,
%вызов \hsinline{tail} с обращением очереди занимает $(m+1)$ реальных
%шагов, но при этом устанавливает хвостовой список равным \hsinline{[]},
%уменьшая таким образом потенциал на $m$, так что амортизированная
%стоимость равна $m + 1 - m = 1$.
%\end{frame}

\begin{frame}{Вывод}

У чисто функциональной очереди функция \mlinline{tail} за $O(n)$ в худшем случае и за $O(1)$ амортизированного.\\

\begin{hint}
  Эта реализация очередей идеальна в приложениях, где не требуется
  устойчивости и где приемлемы амортизированные показатели
  производительности.
\end{hint}

Если совместить ленивые вычисления и амортизированные методы, то можно получить устойчивые очереди с хорошими амортизированными характеристиками.
\end{frame}

\subsection{Ленивые вычисления}
\input{06lazy.tex}

\subsection{Banker's queue}

\begin{frame}{Улучшаем: banker's queue}
\begin{remark}
Эта реализация будет работать за амортизированную $O(1)$ и будет устойчивой
\end{remark}


\begin{enumerate}
\item Вместо списков используем потоки 
\item Явно храним длины
\item Инвариант: $\card{f}>\card{r}$
\end{enumerate}
\vspace{1em}

В момент, когда потоки сравняются под длине конструируем новый $f$ как $f \concat reverse(r)$. \vspace{1em}

Обращение списка 
\begin{itemize}
\item не будет считаться слишком рано из-за ленивости
\item не будет считаться дважды из-за мемоизации
\end{itemize}
\end{frame}


\subsection{Real-time queue}
\begin{frame}{Планирование (scheduling)}
Проблема:
\begin{itemize}
\item Мы делаем $n$ дешёвых вычислений
\item Затем одно дорогое за $O(n)$
\item И из-за этого обязаны заявлять только амортизированную сложность
\end{itemize}

\begin{block}{Идея \emph{планирования (scheduling)}}
Разобьём дорогое вычисление на $n$ составляющих константной стоимости. Каждый раз, выполняя дешёвое вычисление, будем по чуть-чуть выполнять дорогое.
\end{block}
\end{frame}

\begin{frame}{Real-time queue}
Вспоминаем очередь банкира: там мы полагались на вычисление 
$f \mdoubleplus reverse (r)$

Теперь будет использовать для этого специальную функцию \mlinline{rotate}

\[
rotate (f, r, a) =  f \mdoubleplus reverse(r) \mdoubleplus a
\]
Третий параметр -- аккумулятор, будет хранить частичные результаты $reverse(r)$. 
\vspace{1em}

Очевидно, что 
\[
  rotate (f, r, \$Nil) =  f \mdoubleplus reverse(r)
\]
\end{frame}

\begin{frame}{Когда перестраиваем очередь?}
Будем перестраивать очередь, когда $\card{R}\ =\ \card{F} +\ 1$. Это соотношение будет сохраняться на всём протяжении перестроения\\

Докажем соотношение индукцией по длине фронта $\card{F}$.\\

База:
\begin{equation*}
\begin{split}
rotate (\nil, \cons{y}{\nil}, a) 
& \equiv \nil\concat reverse(\cons{y}{\nil}) \concat a \\
& \equiv \cons{y}{a}
\end{split}
\end{equation*}

Переход:
\begin{equation*}
\begin{split}
rotate (\cons{x}{f}, \cons{y}{r}, a) 
& \equiv \cons{x}{f} \concat  reverse(\cons{y}{r}) \concat a \\
& \equiv \cons{x}{f \concat  reverse(\cons{y}{r}) \concat a} \\
& \equiv \cons{x}{f \concat  reverse(r) \concat \cons{y}{a}} \\
& \equiv \cons{x}{rotate (f, r, \cons{y}{a})}
\end{split}
\end{equation*}
\end{frame}

\begin{frame}{Как реализовать $rotate$?}

Напоминаю, мы хотим заменить в очереди банкира $f\concat reverse(r)$ на $rotate(f,r,\nil)$\\

Реализация $rotate(f,r,a)$
\begin{itemize}
\item Если $f\equiv\nil$ и $r\equiv\cons{y}{tl}$, то возвращаем $\cons{y}{a}$
\item Если $f\equiv\cons{x}{f\ '}$ и $r\equiv\cons{y}{r'}$, то возвращаем $\cons{x}{rotate(f\ ', r\ ', \cons{y}{a})}$
\item Другие случаи не возможны из-за инварианта $\card{r}\ =\ \card{f} +\ 1$
\end{itemize}

\begin{remark}
$rotate$ выполняет константное количество вычислений, при этом откладывая вычисление ещё одно вызова $rotate$ от аргументов меньшей длины
\end{remark}
\end{frame}

\def\invariant{$\card{s}\ \equiv\ \card{f}-\card{r}$}
\def\rotate{\ensuremath{rotate}}

\begin{frame}[fragile]{Тип данных для очередей реального времени}

%\begin{minted}{ocaml} 
%type 'a queue = { s : 'a stream; f : 'a stream; r: 'a list } 
%\end{minted}

\begin{itemize}
\item Новое поле s с типом "поток элементов", хранит расписание форсирования вычислений в F
\begin{itemize}
\item Поток s будет суффиксом f, таким что \textbf{все элементы впереди посчитаны до конца}\footnote{Это будет важно при оценке сложности}
\item Форсирование вычислений в f достигается через форсирование головного элемента s
\item Инвариант \invariant
\end{itemize}
\item Перевернутый хвост r конструируется как есть, поэтому это просто список
\item Не храним длины 
\end{itemize}
\end{frame}

\begin{frame}
\begin{notation}
Добавление $x$ в обычный список $xs$ записываем как $x::xs$
\end{notation}
Добавление в очередь $enqueue(f,r,s,x) \equiv queue(f, x::r, S) $

Удаление из очереди: $tail(f,r,s) \equiv queue(f\ ',r,s)$ при $f\equiv\cons{x}{f\ '}$
\vspace{1em}

Дополнительная функция псевдо-конструктор $queue$ поддерживает инвариант \invariant, но в момент вызова аргументы удовлетворяют \invariant+1
\vspace{1em}

Реализация $queue$:
\begin{itemize}
\item Если $s=\cons{x}{s}$, выдаем новую очередь из $f$, $r$ и $s$ (инвариант тривиально сохраняется)
\item Если $s$ пустой, то надо посчитать $f\ ' \equiv rotate(f,r,\nil)$ и вернуть $f\ '$ вместо $f$, $\nil$ и $f\ '$ вместо $s$
\end{itemize}
\end{frame}

\begin{frame}{Про оценку сложности}
Чтобы стоимость была константой необходимо
\begin{itemize}
\item Тратить константу на работу
\item Форсировать вычислений только на константную стоимость
\end{itemize}
\vspace{1em}

Оцениваем:
\begin{itemize}
\item Все конструирования, такие как $\nil$, $\cons{\cdot}{\cdot}$, и тело \rotate{} выполняют константу работы   
\item Вызов \rotate{} форсирует голову фронта, но мы помним, что перед планированием \rotate{} фронт уже был посчитан, так что это тоже константа работы
\end{itemize}
\end{frame}

\begin{frame}{Итоги по очередям}
\begin{center}
\begin{tabular}{ |>{\centering\arraybackslash}p{4cm}|>{\centering\arraybackslash}p{2cm}|>{\centering\arraybackslash}p{3cm}|>{\centering\arraybackslash}p{2.5cm}| } 
 \hline
 Очередь\textbackslash Операция  & enqueue & head & tail \\ \hline
 Банкира  & \Oconst$^*$ & \Oconst{}$^*$ &\Oconst{}$^*$  \\  \hline
 Real-time &  \Oconst & \Oconst{} & \Oconst{}   \\  \hline
\end{tabular}
\end{center}

%Пропуск означает, что точную оценку забыли подсмотреть в литературе

Амортизированные оценки обозначаются с$^*$.
\end{frame}

%\begin{frame}[fragile]{Ленивые вычисления (очень кратко)}
%Можно представлять число списком цифр. Тогда сложение будет работать за $O(n)$ из-за переносов.\\
%
%А ещё можно представить с помощью ленивого варианта списка
%(так называемый \term{поток}{stream}). Как он будет проводить сложение?
%\begin{itemize}
%  \item Вычислит младший разряд за $O(1)$
%  \item Вычисления остальных разрядов проведет потом, если они понадобятся
%\end{itemize}\vspace{.5cm}
%
%N.B. Оценивать сложность алгоритмов в присутствие ленивых вычислений очень сложно.\\
%
%N.B. В языке Haskell \emph{все} вычисления по умолчанию такие.
%
%\end{frame}
%
%
%\ifanswers
%\begin{frame}[fragile]{}
%\begin{exercise}\label{ex:5.1}
%  \textbf{Хогерворд \cite{Hoogerwoord1992}.}  Идея этих очередей легко
%  может быть расширена на абстракцию \term{двусторонней очереди}{double-ended
%    queue}, или \term{дека}{deque}, где чтение и запись разрешены с
%  обоих концов очереди (см. Рис.~\ref{fig:5.3}). Инвариант делается
%  симметричным относительно \lstinline!f! и \lstinline!r!: если
%  очередь содержит более одного элемента, оба списка должны быть
%  непустыми. Когда один из списков становится пустым, мы делим другой
%  пополам и одну из половин обращаем.
%  
%  \begin{enumerate}
%    \item Реализуйте эту версию деков.
%    \item Докажите, что каждая операция занимает $O(1)$ амортизированного
%    времени, используя функцию потенциала $\Phi(f,r) = abs(|f| -
%    |r|)$, где $abs$~--- функция модуля.
%  \end{enumerate}
%\end{exercise}
%\end{frame}
%\fi
%
%\section{Биномиальные кучи и амортизация}
%\label{sc:5.3}
%
%
%\begin{frame}[fragile]{}
%В Разделе~\ref{sc:3.2} мы показали, что вставка в биномиальную кучу
%проходит в худшем случае за время $O(\log n)$. Здесь мы доказываем,
%что на самом деле амортизированное ограничение на время вставки
%составляет $O(1)$.\\
%
%Метод физика. Потенциал биномиальной кучи -- число деревьев в ней. 
%
%Заметим, что это число равно количеству
%единиц в двоичном представлении $n$, числа элементов в куче.  Вызов
%\hsinline{insert} занимает $k+1$ шаг, где $k$~--- число обращений к
%\hsinline{link}. Если изначально в куче было $t$ деревьев, то после
%вставки окажется $t - k + 1$ деревьев. Таким образом, изменение
%потенциала составляет $(t - k + 1) - t = 1 - k$, а амортизированная
%стоимость вставки $(k + 1) + (1 - k) = 2$.\\
%
%\begin{exercise}\label{ex:5.2}
%  Повторите доказательство с использованием метода банкира.
%\end{exercise}
%
%\end{frame}
%
%
%\begin{frame}[fragile]{Можно доказать, что \hsinline{merge} и \hsinline{deleteMin} работают за $O(\log n)$}
%Для полноты картины нам нужно показать, что амортизированная стоимость
%операций \hsinline{merge} и \hsinline{deleteMin} по-прежнему
%составляет $O(\log n)$.\\
%
% \hsinline{deleteMin} не доставляет здесь
%никаких трудностей, но в случае \hsinline{merge} требуется небольшое
%расширение метода физика (нужно учесть, что операции могут возвращать больше одного объекта). \\
%
%%До сих пор мы определяли амортизированную
%%стоимость операции как
%%$$
%%a = t + \Phi(d_{\mbox{\textit{вых}}}) - \Phi(d_{\mbox{\textit{вх}}})
%%$$
%%где $d_{\mbox{\textit{вх}}}$~--- структура на входе операции, а $d_{\mbox{\textit{вых}}}$~---
%%структура на выходе. Однако если операция принимает либо возвращает
%%более одного объекта, это определение требуется обобщить до
%%$$
%%a = t + \sum_{d \in \mbox{\textit{Вых}}} \Phi(d) - \sum_{d \in \mbox{\textit{Вх}}} \Phi(d)
%%$$
%%где $\mbox{\textit{Вх}}$~--- множество входов, а $\mbox{\textit{Вых}}$~--- множество выходов. В этом
%%правиле мы рассматриваем только входы и выходы анализируемого типа.
%\end{frame}
%
%\ifanswers
%\begin{frame}[fragile]{}
%\begin{exercise}\label{ex:5.3}
%  Докажите, что амортизированная стоимость операций \hsinline{merge}
%  и \hsinline{deleteMin} по-прежнему составляет $O(\log n)$.
%\end{exercise}
%\end{frame}
%\fi
%
%
%\section{Расширяющиеся (splay) кучи}
%\label{sc:5.4}
%
%\begin{frame}{\term{Расширяющиеся деревья}{splay trees}}
%\term{Расширяющиеся деревья}{splay trees} \cite{SleatorTarjan1985}~--- возможно, самая известная
%и успешно применяемая амортизированная структура данных.\\
%
% Расширяющиеся
%деревья являются ближайшими родственниками двоичных сбалансированных
%деревьев поиска, но они не хранят никакую информацию о балансе
%явно. \\
%
%Вместо этого каждая операция перестраивает дерево при помощи
%некоторых простых преобразований, которые имеют тенденцию увеличивать
%сбалансированность. Несмотря на то, что каждая конкретная операция
%может занимать до $O(n)$ времени, можно показать, что 
%амортизированная стоимость ее не превышает $O(\log n)$.
%\end{frame}
%
%
%\begin{frame}[fragile]{Расширяющиеся vs. деревья поиска}
%Важное различие между расширяющимися и сбалансированными
%двоичными деревьями поиска вроде красно-чёрных деревьев из
%Раздела~\ref{sc:3.3} состоит в том, что расширяющиеся деревья
%перестраиваются даже во время запросов (таких, как \hsinline{member}),
%а не только во время обновлений (таких, как \hsinline{insert}). \\
%
%Это
%свойство мешает использованию расширяющихся деревьев для реализации
%абстракций вроде множеств или конечных отображений в чисто
%функциональном окружении, поскольку приходилось бы возвращать в
%запросе новое дерево наряду с ответом на запрос\footnote{%
%  В принципе можно было бы хранить корень расширяющегося дерева в
%  ссылочной ячейке и обновлять значение по ссылке при каждом запросе, но
%  такое решение не является чисто функциональным.
%}.
%\end{frame}
%
%
%\begin{frame}[fragile]{}
%Представление расширяющихся деревьев идентично представлению
%несбалансированных двоичных деревьев поиска.
%\inputminted[firstline=5,lastline=5] {haskell}{code/SplayHeap.lhs}
%
%
%Однако в отличие от несбалансированных двоичных деревьев поиска из
%Раздела~\ref{sc:2.2}, мы позволяем дереву содержать повторяющиеся
%элементы. Эта разница не является фундаментальным различием расширяющихся
%деревьев и несбалансированных двоичных деревьев поиска; она просто
%отражает отличие абстракции множества от абстракции кучи.
%
%\end{frame}
%
%
%\begin{frame}[fragile]{Реализация \hsinline{insert} }
%Разобьем существующее дерево на два поддерева, чтобы одно содержало все
%элементы, меньше или равные новому, а второе все элементы, большие
%нового. Затем породим новый узел из нового элемента и двух этих
%поддеревьев. В отличие от вставки в обыкновенное двоичное дерево
%поиска, эта процедура добавляет элемент как корень дерева, а не как
%новый лист.
%
%\begin{minted}{haskell}
%insert x t = T (smaller x t) x (bigger x t)
%\end{minted}
%
%где \hsinline{smaller} выделяет дерево из элементов, меньше или равных
%\hsinline{x}, а \hsinline{bigger} -- больших
%\hsinline{x}. 
%
%\end{frame}
%
%
%\begin{frame}[fragile]{Наивная реализация \hsinline{bigger}}
%По аналогии с фазой разделения быстрой сортировки,
%назовем новый элемент \term{границей}{pivot}.
%
%Можно наивно реализовать \hsinline{bigger} как
%
%\begin{minted}{haskell}
%bigger pivot E = E
%bigger pivot (T a x b) =
%  if x <= pivot 
%  then bigger pivot b
%  else T (bigger pivot a) x b
%\end{minted}
%однако при таком решении не делается никакой попытки перестроить
%дерево, добиваясь лучшего баланса.
%%\begin{minted}{haskell}
%%insert x t = T (smaller (x, t))  x (bigger (x, t))
%%\end{minted}
%\end{frame}
%
%\begin{frame}[fragile]{Правильная реализация \hsinline{bigger} }
%Вместо этого мы применяем простую
%эвристику для перестройки: каждый раз, пройдя по двум левым ветвям
%подряд, мы проворачиваем два пройденных узла.
%
%\begin{minted}{haskell}
%bigger pivot E = E
%bigger pivot (T a x b) =
%  if x <= pivot 
%  then bigger pivot b
%  else case a of
%    E         -> T E x b
%    T a1 y a2 ->
%        if y <= pivot 
%        then T (bigger pivot a2) x b)
%        else T (bigger pivot a1) y (T a2 x b)
%\end{minted} 
%\end{frame}
%
%\begin{frame}[fragile]{}
%\begin{figure}
%  \centering
%  \input{figures/fig.5.4.tex}
%  \caption{Вызов функции \hsinline{bigger} с граничным элементом \hsinline{pivot} = 0 на сильно несбалансированном дереве.}
%  \label{fig:5.4}
%\end{figure}
%
%
%\end{frame}
%
%\begin{frame}[fragile]{}
%На Рис.~\ref{fig:5.4} показано, как \lstinline!bigger! действует на
%сильно несбалансированное дерево. \\
%
%Несмотря на то, что результат
%по-прежнему не является сбалансированным в обычном смысле, новое
%дерево намного сбалансированнее исходного; глубина каждого узла
%уменьшилась примерно наполовину, от $d$ до $\lfloor d/2 \rfloor$ или
%$\lfloor d/2 \rfloor + 1$.\\
%
%Разумеется, мы не всегда можем уполовинить
%глубину каждого узла в дереве, но мы можем уполовинить глубину каждого
%узла, лежащего на пути поиска. \\
%
%В сущности, в этом и состоит принцип
%расширяющихся деревьев: нужно перестраивать путь поиска так, чтобы
%глубина каждого лежащего на пути узла уменьшалась примерно вполовину.
%\end{frame}
%
%\ifanswers
%\begin{frame}[fragile]{}
%\begin{exercise}\label{ex:5.4}
%  Реализуйте операцию \lstinline!smaller!. Не забудьте, что
%  \lstinline!smaller! должна сохранять элементы, равные границе (однако
%  устраивать отдельную проверку на равенство не следует!).
%\end{exercise}
%\end{frame}
%\fi 
%
%
%\begin{frame}[fragile]{}
%Рассмотрим теперь \hsinline{findMin} и
%\hsinline{deleteMin}. Минимальный элемент расширяющегося дерева
%хранится в самой левой его вершине типа \hsinline{T}. Найти эту
%вершину несложно.
%\inputminted[firstline=42,lastline=44,gobble=2] {haskell}{code/SplayHeap.lhs}
%
%Функция \hsinline{deleteMin} должна уничтожить самый левый узел и
%одновременно перестроить дерево таким же образом, как это делает
%\hsinline{bigger}. Поскольку мы всегда рассматриваем только левую
%ветвь, сравнения не нужны.
%\inputminted[firstline=46,lastline=49,gobble=2] {haskell}{code/SplayHeap.lhs}
%
%
%\end{frame}
%
%\begin{frame}[fragile]{}
%N.B. Функция слияния
%\hsinline{merge} довольно неэффективна и для многих входов
%занимает $O(n)$ времени.\\
%
%Можно показать методом физика, что \hsinline{insert} выполняется за время
%$O(\log n)$.
%\end{frame}

\begin{comment}
\begin{frame}[fragile]{}
Теперь мы хотим показать, что \hsinline{insert} выполняется за время
$O(\log n)$. Пусть $\#t$ обозначает размер дерева $t$ плюс
один. Заметим, что если $\hsinline{t = T a x b}$, то $\#t =
\#a + \#b$. Пусть потенциал вершины $\phi(t)$ равен $\log(\# t)$, а
потенциал всего дерева равен сумме потенциалов его вершин. Нам
требуется следующее элементарное утверждение, касающееся логарифмов:
\begin{lemma}\label{lm:5.1}
  Для всех положительных $x, y, z$, таких, что $y + z \le x$,
  $$
  1 + \log y + \log z < 2 \log x
  $$
  
  \noindent
  \textit{Доказательство.} Без потери общности предположим, что $y \le  z$.
  Тогда $y \le x/2$ и $z \le x$, так что $1 + \log y \le \log x$ и
  $\log z < \log x$
\end{lemma}

\end{frame}

\begin{frame}[fragile]{}

Пусть $\mathcal{T}(t)$ обозначает реальную стоимость вызова
\lstinline!partition! для дерева $t$, что определяется как число
рекурсивных вызовов \lstinline!partition!. Пусть $\mathcal{A}(t)$~---
амортизированная стоимость такого вызова, определяемая как
$$
\mathcal{A}(t) = \mathcal{T}(t) + \Phi(a) + \Phi(b) - \Phi(t)
$$
где $a$ и $b$~--- возвращаемые функцией \lstinline!partition!
поддеревья.

\end{frame}


\begin{frame}[fragile]{}
\begin{theorem}\label{th:5.2}
  $\mathcal{A}(t) \le 1 + 2\phi(t) = 1 + 2\log(\#t)$
  
  \noindent\textbf{Доказательство.} Требуется рассмотреть два
  нетривиальных случая, называемые зиг-зиг и зиг-заг, в зависимости
  от того, проходит ли вызов \hsinline{partition} по двум левым
  ветвям (или, симметрично, по двум правым), либо по левой ветке, а
  затем правой (или, симметрично, по правой, а затем по левой).
  
  Для случая зиг-зиг предположим, что исходное и результирующее дерево
  имеют формы
  
  \begin{center}
    \input{figures/formula.theorem.5.2.tex}
  \end{center}
  где $a$ и $b$ являются результатами вызова \hsinline{partition (pivot, u)}.
\end{theorem}
TODO;
\end{frame}

\begin{frame}[fragile]{}
   Тогда
  $$
  \begin{array}{ll}
  & \mathcal{A}(s) \\
  = & \qquad\{\mbox{ по определению $\mathcal{A}$ }\} \\
  & \mathcal{T}(s) + \Phi(a) + \Phi(s') - \Phi(s) \\
  = & \qquad\{\mbox{ $\mathcal{T}(s) = 1 + \mathcal{T}(u)$ }\} \\
  & 1 + \mathcal{T}(u) + \Phi(a) + \Phi(s') - \Phi(s) \\
  = & \qquad\{\mbox{ $\mathcal{T}(u) = \mathcal{A}(u) - \Phi(a) - \Phi(b) + \Phi(u)$ }\} \\
  & 1 + \mathcal{A}(u) - \Phi(a) - \Phi(b) + \Phi(u) + \Phi(a) + \Phi(s') - \Phi(s) \\
  = & \qquad\{\mbox{ раскрываем $\Phi(s)$ и $\Phi(s')$, упрощаем }\} \\
  & 1 + \mathcal{A}(u) + \phi(s') + \phi(t') - \phi(s) - \phi(t) \\
  \le & \qquad\{\mbox{ по предположению индукции, $\mathcal{A}(u) \le 1 + 2\phi(u)$ } \} \\
  & 2 + 2\phi(u) + \phi(s') + \phi(t') - \phi(s) - \phi(t) \\
  < & \qquad \{\mbox{$\phi(u) < \phi(t)$, а $\phi(s') \le \phi(s)$}\} \\
  & 2 + \phi(u) + \phi(t') \\
  < & \qquad \{\mbox{ $\#u + \#t' < \#s$, а также Лемма~\ref{lm:5.1} }\} \\
  & 1 + 2\phi(s) \\
  \end{array}
  $$
  Доказательство случая зиг-заг мы оставляем как упражнение для читателя.
\end{frame}

\begin{frame}[fragile]{}
Дополнительная стоимость операции \hsinline{insert} по сравнению с
\hsinline{partition} составляет один реальный шаг плюс разница
потенциалов между двумя поддеревьями-результатами
\hsinline{partition} и деревом-окончательным результатом
\hsinline{insert}. Это изменение потенциала равно просто $\phi$ от
нового корня. Поскольку амортизированная стоимость
\hsinline{partition} ограничена $1 + 2\log(\#t)$, амортизированная
стоимость \hsinline{insert} ограничена
$2 + 2\log(\#t) + \log(\#t + 1) \approx 2 + 3\log(\#t)$.

TODO
\end{frame}

\end{comment}

