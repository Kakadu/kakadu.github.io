
%\section{Ленивое вычисление}


\chap{Основы амортизации}


\section{Методы амортизированного анализа}
\label{sc:5.1}


\begin{frame}[fragile]{}
Реализации с амортизированными
характеристиками производительности часто оказываются проще и быстрее,
чем реализации со сравнимыми жёсткими характеристиками.

К сожалению, простой подход к амортизации, рассматриваемый в этой
главе, конфликтует с идеей устойчивости~--- эти структуры, будучи
используемы как устойчивые, могут быть весьма неэффективны. Однако на
практике многие приложения устойчивости не требуют, и часто для таких
приложений реализации, представленные в этой главе, могут быть
замечательным выбором. В следующей главе мы увидим, как можно
совместить понятия амортизации и устойчивости при помощи ленивого
вычисления.

TODO:
\end{frame}


\begin{frame}[fragile]{}
Понятие амортизации возникает из следующего наблюдения.  Имея
последовательность операций, мы можем интересоваться временем, которое
отнимает вся эта последовательность, однако при этом нам может быть
безразлично время каждой отдельной операции.\\

 Например, имея $n$
операций, мы можем желать, чтобы время всей последовательности было
ограничено показателем $O(n)$, не настаивая, чтобы каждая из этих
операций происходила за время $O(1)$. Нас может устраивать, чтобы
некоторые из операций занимали $O(\log n)$ или даже $O(n)$, при
условии, что общая стоимость всей последовательности будет
$O(n)$. \\

Такая дополнительная степень свободы открывает широкое
пространство возможностей при проектировании, и часто позволяет найти
более простые и быстрые решения, чем варианты с аналогичными жёсткими
ограничениями.

\end{frame}


\begin{frame}[fragile]{}
$$
\sum_{i=1}^m a_i \ge \sum_{i=1}^m t_i
$$
где $a_i$~--- амортизированная стоимость $i$-й операции, $t_i$~--- ее
реальная стоимость, а $m$~--- общее число операций.\\

 Обычно
доказывается несколько более сильный результат: что на любой
промежуточной стадии в последовательности операций общая текущая
амортизированная стоимость является верхней границей для общей текущей
реальной стоимости, т.~е. для любого $j$
$$
\sum_{i=1}^j a_i \ge \sum_{i=1}^j t_i
$$
\end{frame}


\begin{frame}[fragile]{}
\begin{definition}
Разница между общей текущей амортизированной стоимостью
и общей текущей реальной стоимостью называется
\term{текущие накопления}{accumulated savings}. 
\end{definition}
Таким образом, общая
текущая амортизированная стоимость является верхней границей для
общей текущей реальной стоимости тогда и только тогда, когда текущие
накопления неотрицательны.

\end{frame}


\begin{frame}[fragile]{}
Амортизация позволяет некоторым операциям быть дороже, чем их
амортизированная стоимость. Такие операции называются
\term{дорогими}{expensive}. Операции, для которых амортизированная
стоимость превышает реальную, называются
\term{дешевыми}{cheap}. Дорогие операции уменьшают текущие накопления,
а дешевые их увеличивают.\\

 Главное при доказательстве
амортизированных характеристик стоимости~--- показать, что дорогие
операции случаются только тогда, когда текущих накоплений хватает,
чтобы покрыть их дополнительную стоимость.
\end{frame}



\begin{frame}[fragile]{}
\begin{itemize}
  \item \term{Метод банкира}{banker's method} 
      \begin{itemize}
        \item \term{кредит}{credits}
      \end{itemize}
  \item \term{Метод физика}{physicist's method}
      \begin{itemize}
        \item \term{потенциал}{potential}
      \end{itemize}
\end{itemize}



Кредит и потенциал являются лишь средствами анализа; ни
то, ни другое не присутствует в тексте программы (разве что, возможно,
в комментариях).

\end{frame}

\begin{frame}[fragile]{}
 В методе банкира
текущие накопления представляются как \term{кредит}{credits},
привязанный к определенным ячейкам структуры данных. Этот кредит
используется, чтобы расплатиться за будущие операции доступа к этим
ячейкам.  Амортизированная стоимость операции определяется как ее
реальная стоимость плюс размер кредита, выделяемого этой операцией,
минус размер кредита, который она расходует, т.~е.,
$$
a_i = t_i + c_i - \bar{c}_i
$$
где $c_i$~--- размер кредита, выделяемого операцией $i$, а $\bar{c}_i$~---
размер кредита, расходуемого операцией $i$.


\end{frame}

\begin{frame}[fragile]{}
$$
a_i = t_i + c_i - \bar{c}_i
$$
где $c_i$~--- размер кредита, выделяемого операцией $i$, а $\bar{c}_i$~---
размер кредита, расходуемого операцией $i$.\\

 Каждая единица кредита
должна быть выделена, прежде чем израсходована, и нельзя расходовать
кредит дважды. Таким образом, $\sum c_i \ge \sum \bar{c}_i$, а
следовательно, как и требуется, $\sum a_i \ge \sum t_i$.\\

Как правило,
доказательства с использованием метода банкира определяют
\term{инвариант кредита}{credit invariant}, регулирующий распределение
кредита так, чтобы при всякой дорогой операции достаточное его
количество было выделено в нужных ячейках структуры для покрытия
стоимости операции.
\end{frame}


\begin{frame}[fragile]{}
1
\end{frame}

\begin{frame}[fragile]{Метод физика}
Определяется функция $\Phi$, отображающая всякий
объект $d$ на действительное число, называемое его
\term{потенциалом}{potential}.  Потенциал обычно выбирается так, чтобы
изначально равняться нулю и оставаться неотрицательным. В таком случае
потенциал представляет нижнюю границу  текущих накоплений.\\

Пусть объект $d_i$ будет результатом операции $i$ и аргументом
операции $i+1$. Тогда амортизированная стоимость операции $i$
определяется как сумма реальной стоимости и изменения потенциалов между
$d_{i-1}$ и $d_i$, т.~е.,
$$
a_i = t_i + \Phi(d_i) - \Phi(d_{i-1})
$$
текущих накоплений.


\end{frame}


\begin{frame}[fragile]{}
$$
a_i = t_i + \Phi(d_i) - \Phi(d_{i-1})
$$
Текущая реальная стоимость последовательности операций равна
$$
\begin{array}{rcl}
\sum_{i=1}^j t_i & = & \sum_{i=0}^j (a_i + \Phi(d_{i-1}) - \Phi(d_i))\\
\\
& = & \sum_{i=1}^j a_i + \sum_{i=1}^j (\Phi(d_{i-1}) - \Phi(d_i)) \\
\\
& = & \sum_{i=1}^j a_i + \Phi(d_0) - \Phi(d_j)
\end{array}
$$

Если $\Phi$ выбран таким образом, что
$\Phi(d_0)$ равен нулю, а $\Phi(d_j)$ неотрицателен, мы имеем
$\Phi(d_j) \ge \Phi(d_0)$, так что, как и требуется, текущая общая
амортизированная стоимость является верхней границей для текущей общей
реальной стоимости.

\end{frame}

\begin{comment}
\begin{remark}
Такое описание метода физика несколько упрощает
картину. Часто при анализе оказывается трудно втиснуть реальное
положение дел в указанные рамки. Например, что делать с функциями,
которые порождают или возвращают более одного объекта? Однако даже
упрощенное описание достаточно для демонстрации основных идей.
\end{remark}

\end{comment}

\section{Очереди}
\label{sc:5.2}


\begin{frame}[fragile]{}

\begin{minipage}{.4\textwidth}
  \inputminted[firstline=5,lastline=11] {haskell}{code/Queue.lhs}
\end{minipage}
\begin{minipage}{.55\textwidth}
  Самая распространенная чисто функциональная реализация очередей
  представляет собой пару списков, \hsinline{f} и \hsinline{r}, где
  \hsinline{f} содержит головные элементы очереди в правильном порядке,
  а \hsinline{r} состоит из хвостовых элементов в обратном порядке.\\
  
  Например, очередь, содержащая целые числа 1\ldots 6, может быть
  представлена списками \hsinline{f=[1,2,3]} и
  \hsinline{r=[6,5,4]}. Это представление можно описать следующим
  типом:
  \begin{minted}{haskell}
  data Queue a = Queue [a] [a]
  \end{minted}
  
\end{minipage}
\end{frame}


\begin{frame}[fragile]{}
В этом представлении голова очереди~--- первый элемент \hsinline{f},
так что функции \hsinline{head} и \hsinline{tail}
возвращают и отбрасывают этот элемент, соответственно.
\begin{minted}{haskell}
head (x : f, r) = x
tail (x : f, r) = f
\end{minted}
Подобным образом, хвостом очереди является первый элемент
\hsinline{r}, так что \hsinline{snoc} добавляет к \hsinline{r}
новый.
\begin{minted}{haskell}
snoc (f,r) x = (f, x : r)
\end{minted}

\end{frame}


\begin{frame}[fragile]{}
Элементы добавляются к \hsinline{r} и убираются из \hsinline{f}, так
что они должны как-то переезжать из одного списка в другой. Этот
переезд осуществляется путем обращения \hsinline{r} и установки его
на место \hsinline{f} всякий раз, когда в противном случае
\hsinline{f} оказался бы пустым.\\

 Одновременно \hsinline{r}
устанавливается в \hsinline{[]}. Наша цель~--- поддерживать
инвариант, что список \hsinline{f} может быть пустым только в том
случае, когда список \hsinline{r} также пуст (т.~е., пуста вся
очередь). \\

Заметим, что если бы \hsinline{f} был пустым при непустом
\hsinline{r}, то первый элемент очереди находился бы в конце
\hsinline{r}, и доступ к нему занимал бы $O(n)$ времени. Поддерживая
инвариант, мы гарантируем, что функция \hsinline{head} всегда может
найти голову очереди за $O(1)$ времени.

\end{frame}


\begin{frame}[fragile]{}
\begin{minted}{haskell}
snoc (([], _), x) = ([x], [])
snoc (( f, r), x) = (f,  x :: r)
tail ([x], r) = (rev r, [])
tail (x:f, r) = (f, r)
\end{minted}

Заметим, что в первой ветке \hsinline{snoc} используется
wildcard. В этом случае поле \hsinline{r} проверять не нужно,
поскольку из инварианта мы знаем, что если список \hsinline{f} равен
\hsinline{[]}, то \hsinline{r} также пуст.

\end{frame}


\begin{frame}[fragile]{}
Чуть более изящный способ записать эти функции~--- вынести те части
\hsinline{snoc} и \hsinline{tail}, которые поддерживают инвариант, в
отдельную функцию \hsinline{checkf}. Она заменяет \hsinline{f} на
\hsinline{rev r}, если \hsinline{f} пуст, а в противном случае
ничего не делает.

\inputminted[firstline=10,lastline=15] {haskell}{code/NaiveQueue.hs}

Функции
\hsinline{snoc} и \hsinline{head} всегда завершаются за время
$O(1)$, но \hsinline{tail} в худшем случае отнимает $O(n)$
времени. 

Однако, используя либо метод банкира, либо метод физика, мы
можем показать, что как \hsinline{snoc}, так и \hsinline{tail}
занимают амортизированное время $O(1)$.

\end{frame}


\begin{frame}[fragile]{Чисто функциональная очередь и метод банкира}

Инвариант: каждый элемент в
хвостовом списке связан с одной единицей кредита. \\

Каждый вызов
\hsinline{snoc} для непустой очереди занимает один реальный шаг и
выделяет одну единицу кредита для элемента хвостового списка; таким
образом, общая амортизированная стоимость равна двум. \\

Вызов
\hsinline{tail}, не обращающий хвостовой список, занимает один шаг,
не выделяет и не тратит никакого кредита, и, таким образом, имеет
амортизированную стоимость 1. \\

Наконец, вызов \hsinline{tail},
обращающий хвостовой список, занимает $m+1$ реальный шаг, где $m$~---
длина хвостового списка, и тратит $m$ единиц кредита, содержащиеся в
этом списке, так что амортизированная стоимость получается $m + 1 - m
= 1$.

\end{frame}


\begin{frame}[fragile]{Чисто функциональная очередь и метод физика}
В методе физика мы определяем функцию потенциала $\Phi$ как длину
хвостового списка. \\

Тогда всякий \hsinline{snoc} к непустой очереди
занимает один реальный шаг и увеличивает потенциал на единицу, так что
амортизированная стоимость равна двум. \\

Вызов \hsinline{tail} без
обращения хвостовой очереди занимает один реальный шаг и не изменяет
потенциал, так что амортизированная стоимость равна одному.\\

 Наконец,
вызов \hsinline{tail} с обращением очереди занимает $m+1$ реальный
шаг, но при этом устанавливает хвостовой список равным \hsinline{[]},
уменьшая таким образом потенциал на $m$, так что амортизированная
стоимость равна $m + 1 - m = 1$.

\end{frame}


\begin{frame}[fragile]{}
\begin{hint}
  Эта реализация очередей идеальна в приложениях, где не требуется
  устойчивости и где приемлемы амортизированные показатели
  производительности.
\end{hint}

\end{frame}

\ifanswers
\begin{frame}[fragile]{}
\begin{exercise}\label{ex:5.1}
  \textbf{Хогерворд \cite{Hoogerwoord1992}.}  Идея этих очередей легко
  может быть расширена на абстракцию \term{двусторонней очереди}{double-ended
    queue}, или \term{дека}{deque}, где чтение и запись разрешены с
  обоих концов очереди (см. Рис.~\ref{fig:5.3}). Инвариант делается
  симметричным относительно \lstinline!f! и \lstinline!r!: если
  очередь содержит более одного элемента, оба списка должны быть
  непустыми. Когда один из списков становится пустым, мы делим другой
  пополам и одну из половин обращаем.
  
  \begin{enumerate}
    \item Реализуйте эту версию деков.
    \item Докажите, что каждая операция занимает $O(1)$ амортизированного
    времени, используя функцию потенциала $\Phi(f,r) = abs(|f| -
    |r|)$, где $abs$~--- функция модуля.
  \end{enumerate}
\end{exercise}
\end{frame}
\fi

\section{Биномиальные кучи}
\label{sc:5.3}


\begin{frame}[fragile]{}
В Разделе~\ref{sc:3.2} мы показали, что вставка в биномиальную кучу
проходит в худшем случае за время $O(\log n)$. Здесь мы доказываем,
что на самом деле амортизированное ограничение на время вставки
составляет $O(1)$.\\

Метод физика. Потенциал биномиальной кучи -- число деревьев в ней. 

Заметим, что это число равно количеству
единиц в двоичном представлении $n$, числа элементов в куче.  Вызов
\hsinline{insert} занимает $k+1$ шаг, где $k$~--- число обращений к
\hsinline{link}. Если изначально в куче было $t$ деревьев, то после
вставки окажется $t - k + 1$ деревьев. Таким образом, изменение
потенциала составляет $(t - k + 1) - t = 1 - k$, а амортизированная
стоимость вставки $(k + 1) - (1 - k) = 2$.\\

\begin{exercise}\label{ex:5.2}
  Повторите доказательство с использованием метода банкира.
\end{exercise}

\end{frame}


\begin{frame}[fragile]{}
Для полноты картины нам нужно показать, что амортизированная стоимость
операций \hsinline{merge} и \hsinline{deleteMin} по-прежнему
составляет $O(\log n)$. \hsinline{deleteMin} не доставляет здесь
никаких трудностей, но в случае \hsinline{merge} требуется небольшое
расширение метода физика. \\

До сих пор мы определяли амортизированную
стоимость операции как
$$
a = t + \Phi(d_{\mbox{\textit{вых}}}) - \Phi(d_{\mbox{\textit{вх}}})
$$
где $d_{\mbox{\textit{вх}}}$~--- структура на входе операции, а $d_{\mbox{\textit{вых}}}$~---
структура на выходе. Однако если операция принимает либо возвращает
более одного объекта, это определение требуется обобщить до
$$
a = t + \sum_{d \in \mbox{\textit{Вых}}} \Phi(d) - \sum_{d \in \mbox{\textit{Вх}}} \Phi(d)
$$
где $\mbox{\textit{Вх}}$~--- множество входов, а $\mbox{\textit{Вых}}$~--- множество выходов. В этом
правиле мы рассматриваем только входы и выходы анализируемого типа.
\end{frame}

\ifanswers
\begin{frame}[fragile]{}
\begin{exercise}\label{ex:5.3}
  Докажите, что амортизированная стоимость операций \hsinline{merge}
  и \hsinline{deleteMin} по-прежнему составляет $O(\log n)$.
\end{exercise}
\end{frame}
\fi


\section{Расширяющиеся кучи}
\label{sc:5.4}

\begin{frame}[fragile]{}
\term{Расширяющиеся деревья}{splay trees} \cite{SleatorTarjan1985}~--- возможно, самая известная
и успешно применяемая амортизированная структура данных.\\

 Расширяющиеся
деревья являются ближайшими родственниками двоичных сбалансированных
деревьев поиска, но они не хранят никакую информацию о балансе
явно. \\

Вместо этого каждая операция перестраивает дерево при помощи
некоторых простых преобразований, которые имеют тенденцию увеличивать
сбалансированность. Несмотря на то, что каждая конкретная операция
может занимать до $O(n)$ времени, амортизированная стоимость ее, как
мы покажем, не превышает $O(\log n)$.
\end{frame}


\begin{frame}[fragile]{}
Важное различие между расширяющимися и сбалансированными
двоичными деревьями поиска вроде красно-чёрных деревьев из
Раздела~\ref{sc:3.3} состоит в том, что расширяющиеся деревья
перестраиваются даже во время запросов (таких, как \hsinline{member}),
а не только во время обновлений (таких, как \hsinline{insert}). \\

Это
свойство мешает использованию расширяющихся деревьев для реализации
абстракций вроде множеств или конечных отображений в чисто
функциональном окружении, поскольку приходилось бы возвращать в
запросе новое дерево наряду с ответом на запрос\footnote{%
  В принципе можно было бы хранить корень расширяющегося дерева в
  ссылочной ячейке и обновлять значение по ссылке при каждом запросе, но
  такое решение не является чисто функциональным.
}
\end{frame}


\begin{frame}[fragile]{}
Представление расширяющихся деревьев идентично представлению
несбалансированных двоичных деревьев поиска.
\inputminted[firstline=5,lastline=5] {haskell}{code/SplayHeap.lhs}


Однако в отличие от несбалансированных двоичных деревьев поиска из
Раздела~\ref{sc:2.2}, мы позволяем дереву содержать повторяющиеся
элементы. Эта разница не является фундаментальным различием расширяющихся
деревьев и несбалансированных двоичных деревьев поиска; она просто
отражает отличие абстракции множества от абстракции кучи.

\end{frame}


\begin{frame}[fragile]{Реализация \hsinline{insert} }
Разобьем существующее дерево на два поддерева, чтобы одно содержало все
элементы, меньше или равные новому, а второе все элементы, большие
нового. Затем породим новый узел из нового элемента и двух этих
поддеревьев. В отличие от вставки в обыкновенное двоичное дерево
поиска, эта процедура добавляет элемент как корень дерева, а не как
новый лист.

\begin{minted}{haskell}
insert x t = T (smaller x t) x (bigger x t)
\end{minted}

где \hsinline{smaller} выделяет дерево из элементов, меньше или равных
\hsinline{x}, а \hsinline{bigger} -- больших
\hsinline{x}. 

\end{frame}


\begin{frame}[fragile]{Наивная реализация \hsinline{bigger}}
По аналогии с фазой разделения быстрой сортировки,
назовем новый элемент \term{границей}{pivot}.

Можно наивно реализовать \hsinline{bigger} как

\begin{minted}{haskell}
bigger pivot E = E
bigger pivot (T a x b) =
  if x <= pivot 
  then bigger pivot b
  else T (bigger pivot a) x b
\end{minted}
однако при таком решении не делается никакой попытки перестроить
дерево, добиваясь лучшего баланса.
%\begin{minted}{haskell}
%insert x t = T (smaller (x, t))  x (bigger (x, t))
%\end{minted}
\end{frame}

\begin{frame}[fragile]{Правильная реализация \hsinline{bigger} }
Вместо этого мы применяем простую
эвристику для перестройки: каждый раз, пройдя по двум левым ветвям
подряд, мы проворачиваем два пройденных узла.

\begin{minted}{haskell}
bigger pivot E = E
bigger pivot (T a x b) =
  if x <= pivot 
  then bigger pivot b
  else case a of
    E         -> T E x b
    T a1 y a2 ->
        if y <= pivot 
        then T (bigger pivot a2) x b)
        else T (bigger pivot a1) y (T a2 x b)
\end{minted} 
\end{frame}

\begin{frame}[fragile]{}
\begin{figure}
  \centering
  \input{figures/fig.5.4.tex}
  \caption{Вызов функции \hsinline{bigger} с граничным элементом 0 на сильно несбалансированном дереве.}
  \label{fig:5.4}
\end{figure}


\end{frame}

\begin{frame}[fragile]{}
На Рис.~\ref{fig:5.4} показано, как \lstinline!bigger! действует на
сильно несбалансированное дерево. \\

Несмотря на то, что результат
по-прежнему не является сбалансированным в обычном смысле, новое
дерево намного сбалансированнее исходного; глубина каждого узла
уменьшилась примерно наполовину, от $d$ до $\lfloor d/2 \rfloor$ или
$\lfloor d/2 \rfloor + 1$.\\

Разумеется, мы не всегда можем уполовинить
глубину каждого узла в дереве, но мы можем уполовинить глубину каждого
узла, лежащего на пути поиска. \\

В сущности, в этом и состоит принцип
расширяющихся деревьев: нужно перестраивать путь поиска так, чтобы
глубина каждого лежащего на пути узла уменьшалась примерно вполовину.
\end{frame}

\begin{frame}[fragile]{}
\begin{exercise}\label{ex:5.4}
  Реализуйте операцию \lstinline!smaller!. Не забудьте, что
  \lstinline!smaller! должна сохранять элементы, равные границе (однако
  устраивать отдельную проверку на равенство не следует!).
\end{exercise}

\end{frame}

\begin{frame}[fragile]{}

Рассмотрим теперь \hsinline{findMin} и
\hsinline{deleteMin}. Минимальный элемент расширяющегося дерева
хранится в самой левой его вершине типа \hsinline{T}. Найти эту
вершину несложно.
\inputminted[firstline=42,lastline=44,gobble=2] {haskell}{code/SplayHeap.lhs}

Функция \hsinline{deleteMin} должна уничтожить самый левый узел и
одновременно перестроить дерево таким же образом, как это делает
\hsinline{bigger}. Поскольку мы всегда рассматриваем только левую
ветвь, сравнения не нужны.
\inputminted[firstline=46,lastline=49,gobble=2] {haskell}{code/SplayHeap.lhs}

N.B. Функция слияния
\hsinline{merge} довольно неэффективна и для многих входов
занимает $O(n)$ времени.


\end{frame}

\begin{frame}[fragile]{}
Теперь мы хотим показать, что \hsinline{insert} выполняется за время
$O(\log n)$. Пусть $\#t$ обозначает размер дерева $t$ плюс
один. Заметим, что если $\hsinline{t = T a x b}$, то $\#t =
\#a + \#b$. Пусть потенциал вершины $\phi(t)$ равен $\log(\# t)$, а
потенциал всего дерева равен сумме потенциалов его вершин. Нам
требуется следующее элементарное утверждение, касающееся логарифмов:
\begin{lemma}\label{lm:5.1}
  Для всех положительных $x, y, z$, таких, что $y + z \le x$,
  $$
  1 + \log y + \log z < 2 \log x
  $$
  
  \noindent
  \textit{Доказательство.} Без потери общности предположим, что $y \le  z$.
  Тогда $y \le x/2$ и $z \le x$, так что $1 + \log y \le \log x$ и
  $\log z < \log x$
\end{lemma}

\end{frame}

\begin{frame}[fragile]{}

Пусть $\mathcal{T}(t)$ обозначает реальную стоимость вызова
\lstinline!partition! для дерева $t$, что определяется как число
рекурсивных вызовов \lstinline!partition!. Пусть $\mathcal{A}(t)$~---
амортизированная стоимость такого вызова, определяемая как
$$
\mathcal{A}(t) = \mathcal{T}(t) + \Phi(a) + \Phi(b) - \Phi(t)
$$
где $a$ и $b$~--- возвращаемые функцией \lstinline!partition!
поддеревья.

\end{frame}


\begin{frame}[fragile]{}
\begin{theorem}\label{th:5.2}
  $\mathcal{A}(t) \le 1 + 2\phi(t) = 1 + 2\log(\#t)$
  
  \noindent\textbf{Доказательство.} Требуется рассмотреть два
  нетривиальных случая, называемые зиг-зиг и зиг-заг, в зависимости
  от того, проходит ли вызов \hsinline{partition} по двум левым
  ветвям (или, симметрично, по двум правым), либо по левой ветке, а
  затем правой (или, симметрично, по правой, а затем по левой).
  
  Для случая зиг-зиг предположим, что исходное и результирующее дерево
  имеют формы
  
  \begin{center}
    \input{figures/formula.theorem.5.2.tex}
  \end{center}
  где $a$ и $b$ являются результатами вызова \hsinline{partition (pivot, u)}.
\end{theorem}
TODO;
\end{frame}

\begin{frame}[fragile]{}
   Тогда
  $$
  \begin{array}{ll}
  & \mathcal{A}(s) \\
  = & \qquad\{\mbox{ по определению $\mathcal{A}$ }\} \\
  & \mathcal{T}(s) + \Phi(a) + \Phi(s') - \Phi(s) \\
  = & \qquad\{\mbox{ $\mathcal{T}(s) = 1 + \mathcal{T}(u)$ }\} \\
  & 1 + \mathcal{T}(u) + \Phi(a) + \Phi(s') - \Phi(s) \\
  = & \qquad\{\mbox{ $\mathcal{T}(u) = \mathcal{A}(u) - \Phi(a) - \Phi(b) + \Phi(u)$ }\} \\
  & 1 + \mathcal{A}(u) - \Phi(a) - \Phi(b) + \Phi(u) + \Phi(a) + \Phi(s') - \Phi(s) \\
  = & \qquad\{\mbox{ раскрываем $\Phi(s)$ и $\Phi(s')$, упрощаем }\} \\
  & 1 + \mathcal{A}(u) + \phi(s') + \phi(t') - \phi(s) - \phi(t) \\
  \le & \qquad\{\mbox{ по предположению индукции, $\mathcal{A}(u) \le 1 + 2\phi(u)$ } \} \\
  & 2 + 2\phi(u) + \phi(s') + \phi(t') - \phi(s) - \phi(t) \\
  < & \qquad \{\mbox{$\phi(u) < \phi(t)$, а $\phi(s') \le \phi(s)$}\} \\
  & 2 + \phi(u) + \phi(t') \\
  < & \qquad \{\mbox{ $\#u + \#t' < \#s$, а также Лемма~\ref{lm:5.1} }\} \\
  & 1 + 2\phi(s) \\
  \end{array}
  $$
  Доказательство случая зиг-заг мы оставляем как упражнение для читателя.
\end{frame}

\begin{frame}[fragile]{}
Дополнительная стоимость операции \hsinline{insert} по сравнению с
\hsinline{partition} составляет один реальный шаг плюс разница
потенциалов между двумя поддеревьями-результатами
\hsinline{partition} и деревом-окончательным результатом
\hsinline{insert}. Это изменение потенциала равно просто $\phi$ от
нового корня. Поскольку амортизированная стоимость
\hsinline{partition} ограничена $1 + 2\log(\#t)$, амортизированная
стоимость \hsinline{insert} ограничена
$2 + 2\log(\#t) + \log(\#t + 1) \approx 2 + 3\log(\#t)$.

TODO
\end{frame}
